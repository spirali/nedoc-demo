<!DOCTYPE html><html><head><meta charset=utf-8><title>distributed.protocol.numba</title><meta name=viewport content="width=device-width, initial-scale=1.0"><link rel=stylesheet href=./assets/purecss/pure-min.css><link rel=stylesheet href=./assets/purecss/grids-responsive-min.css><link rel=stylesheet href=./assets/style.css><link rel=stylesheet href=./assets/pygments/default.css><link rel=stylesheet href=./assets/jquery/jquery-ui.min.css><script src=./assets/jquery/jquery-3.0.0.min.js></script><script src=./assets/jquery/jquery-ui.min.js></script><script src=nedoc.js></script></head><body><div id=layout class=pure-g><div class="sidebar pure-u-1 pure-u-md-1-4"><div class=header><h1 class=brand-title>Distributed</h1> 1.0 </div><div id=sbox><input id=search style="color: black;" placeholder="search ..."></div><div class=tree><ul><li><a href=distributed.html>&#9662; distributed</a> </li><li><ul><li><a href=distributed.actor.html>&#9656; actor</a> </li><li><a href=distributed.asyncio.html>&#9656; asyncio</a> </li><li><a href=distributed.batched.html>&#9656; batched</a> </li><li><a href=distributed.bokeh.html>&#9656; bokeh</a> </li><li><a href=distributed.cfexecutor.html>&#9656; cfexecutor</a> </li><li><a href=distributed.cli.html>&#9656; cli</a> </li><li><a href=distributed.client.html>&#9656; client</a> </li><li><a href=distributed.comm.html>&#9656; comm</a> </li><li><a href=distributed.compatibility.html>&#9656; compatibility</a> </li><li><a href=distributed.config.html>&#9656; config</a> </li><li><a href=distributed.core.html>&#9656; core</a> </li><li><a href=distributed.counter.html>&#9656; counter</a> </li><li><a href=distributed.dashboard.html>&#9656; dashboard</a> </li><li><a href=distributed.deploy.html>&#9656; deploy</a> </li><li><a href=distributed.diagnostics.html>&#9656; diagnostics</a> </li><li><a href=distributed.diskutils.html>&#9656; diskutils</a> </li><li><a href=distributed.event.html>&#9656; event</a> </li><li><a href=distributed.http.html>&#9656; http</a> </li><li><a href=distributed.lock.html>&#9656; lock</a> </li><li><a href=distributed.locket.html>&#9656; locket</a> </li><li><a href=distributed.metrics.html>&#9656; metrics</a> </li><li><a href=distributed.nanny.html>&#9656; nanny</a> </li><li><a href=distributed.node.html>&#9656; node</a> </li><li><a href=distributed.preloading.html>&#9656; preloading</a> </li><li><a href=distributed.process.html>&#9656; process</a> </li><li><a href=distributed.proctitle.html>&#9656; proctitle</a> </li><li><a href=distributed.profile.html>&#9656; profile</a> </li><li><a href=distributed.protocol.html>&#9662; protocol</a> </li><li><ul><li><a href=distributed.protocol.arrow.html>&#9656; arrow</a> </li><li><a href=distributed.protocol.compression.html>&#9656; compression</a> </li><li><a href=distributed.protocol.core.html>&#9656; core</a> </li><li><a href=distributed.protocol.cuda.html>&#9656; cuda</a> </li><li><a href=distributed.protocol.cupy.html>&#9656; cupy</a> </li><li><a href=distributed.protocol.h5py.html>&#9656; h5py</a> </li><li><a href=distributed.protocol.highlevelgraph.html>&#9656; highlevelgraph</a> </li><li><a href=distributed.protocol.keras.html>&#9656; keras</a> </li><li><a href=distributed.protocol.netcdf4.html>&#9656; netcdf4</a> </li><li><div class=select><a href=distributed.protocol.numba.html>&#9662; numba</a> </div></li><li><a href=distributed.protocol.numpy.html>&#9656; numpy</a> </li><li><a href=distributed.protocol.pickle.html>&#9656; pickle</a> </li><li><a href=distributed.protocol.rmm.html>&#9656; rmm</a> </li><li><a href=distributed.protocol.scipy.html>&#9656; scipy</a> </li><li><a href=distributed.protocol.serialize.html>&#9656; serialize</a> </li><li><a href=distributed.protocol.sparse.html>&#9656; sparse</a> </li><li><a href=distributed.protocol.tests.html>&#9656; tests</a> </li><li><a href=distributed.protocol.torch.html>&#9656; torch</a> </li><li><a href=distributed.protocol.utils.html>&#9656; utils</a> </li></ul></li><li><a href=distributed.publish.html>&#9656; publish</a> </li><li><a href=distributed.pubsub.html>&#9656; pubsub</a> </li><li><a href=distributed.pytest_resourceleaks.html>&#9656; pytest_resourceleaks</a> </li><li><a href=distributed.queues.html>&#9656; queues</a> </li><li><a href=distributed.recreate_exceptions.html>&#9656; recreate_exceptions</a> </li><li><a href=distributed.scheduler.html>&#9656; scheduler</a> </li><li><a href=distributed.security.html>&#9656; security</a> </li><li><a href=distributed.semaphore.html>&#9656; semaphore</a> </li><li><a href=distributed.sizeof.html>&#9656; sizeof</a> </li><li><a href=distributed.stealing.html>&#9656; stealing</a> </li><li><a href=distributed.system.html>&#9656; system</a> </li><li><a href=distributed.system_monitor.html>&#9656; system_monitor</a> </li><li><a href=distributed.tests.html>&#9656; tests</a> </li><li><a href=distributed.threadpoolexecutor.html>&#9656; threadpoolexecutor</a> </li><li><a href=distributed.utils.html>&#9656; utils</a> </li><li><a href=distributed.utils_comm.html>&#9656; utils_comm</a> </li><li><a href=distributed.utils_perf.html>&#9656; utils_perf</a> </li><li><a href=distributed.utils_test.html>&#9656; utils_test</a> </li><li><a href=distributed.variable.html>&#9656; variable</a> </li><li><a href=distributed.versions.html>&#9656; versions</a> </li><li><a href=distributed.worker.html>&#9656; worker</a> </li><li><a href=distributed.worker_client.html>&#9656; worker_client</a> </li></ul></ul></div></div><div class="content pure-u-1 pure-u-md-3-4"><h1>Source code distributed/protocol/numba.py</h1><div id=path><a class=symbol href=distributed.html>distributed</a>.<a class=symbol href=distributed.protocol.html>protocol</a>.<a class=symbol href=distributed.protocol.numba.html>numba</a></div><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69</pre></div></td><td class=code><div class=highlight><pre><span></span><a name=line-1></a><span class=kn>import</span> <span class=nn>weakref</span>
<a name=line-2></a>
<a name=line-3></a><span class=kn>import</span> <span class=nn>numba.cuda</span>
<a name=line-4></a><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<a name=line-5></a>
<a name=line-6></a><span class=kn>from</span> <span class=nn>.cuda</span> <span class=kn>import</span> <span class=n>cuda_deserialize</span><span class=p>,</span> <span class=n>cuda_serialize</span>
<a name=line-7></a><span class=kn>from</span> <span class=nn>.serialize</span> <span class=kn>import</span> <span class=n>dask_deserialize</span><span class=p>,</span> <span class=n>dask_serialize</span>
<a name=line-8></a>
<a name=line-9></a><span class=k>try</span><span class=p>:</span>
<a name=line-10></a>    <span class=kn>from</span> <span class=nn>.rmm</span> <span class=kn>import</span> <span class=n>dask_deserialize_rmm_device_buffer</span>
<a name=line-11></a><span class=k>except</span> <span class=ne>ImportError</span><span class=p>:</span>
<a name=line-12></a>    <span class=n>dask_deserialize_rmm_device_buffer</span> <span class=o>=</span> <span class=kc>None</span>
<a name=line-13></a>
<a name=line-14></a>
<a name=line-15></a><span class=nd>@cuda_serialize</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>devicearray</span><span class=o>.</span><span class=n>DeviceNDArray</span><span class=p>)</span>
<a name=line-16></a><span class=k>def</span> <span class=nf>cuda_serialize_numba_ndarray</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
<a name=line-17></a>    <span class=c1># Making sure `x` is behaving</span>
<a name=line-18></a>    <span class=k>if</span> <span class=ow>not</span> <span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>flags</span><span class=p>[</span><span class=s2>&quot;C_CONTIGUOUS&quot;</span><span class=p>]</span> <span class=ow>or</span> <span class=n>x</span><span class=o>.</span><span class=n>flags</span><span class=p>[</span><span class=s2>&quot;F_CONTIGUOUS&quot;</span><span class=p>]):</span>
<a name=line-19></a>        <span class=n>shape</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
<a name=line-20></a>        <span class=n>t</span> <span class=o>=</span> <span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>device_array</span><span class=p>(</span><span class=n>shape</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
<a name=line-21></a>        <span class=n>t</span><span class=o>.</span><span class=n>copy_to_device</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
<a name=line-22></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>t</span>
<a name=line-23></a>
<a name=line-24></a>    <span class=n>header</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>__cuda_array_interface__</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
<a name=line-25></a>    <span class=n>header</span><span class=p>[</span><span class=s2>&quot;strides&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=nb>tuple</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>strides</span><span class=p>)</span>
<a name=line-26></a>    <span class=n>header</span><span class=p>[</span><span class=s2>&quot;lengths&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=n>x</span><span class=o>.</span><span class=n>nbytes</span><span class=p>]</span>
<a name=line-27></a>    <span class=n>frames</span> <span class=o>=</span> <span class=p>[</span>
<a name=line-28></a>        <span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>cudadrv</span><span class=o>.</span><span class=n>devicearray</span><span class=o>.</span><span class=n>DeviceNDArray</span><span class=p>(</span>
<a name=line-29></a>            <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>nbytes</span><span class=p>,),</span> <span class=n>strides</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>dtype</span><span class=p>(</span><span class=s2>&quot;u1&quot;</span><span class=p>),</span> <span class=n>gpu_data</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>gpu_data</span>
<a name=line-30></a>        <span class=p>)</span>
<a name=line-31></a>    <span class=p>]</span>
<a name=line-32></a>
<a name=line-33></a>    <span class=k>return</span> <span class=n>header</span><span class=p>,</span> <span class=n>frames</span>
<a name=line-34></a>
<a name=line-35></a>
<a name=line-36></a><span class=nd>@cuda_deserialize</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>devicearray</span><span class=o>.</span><span class=n>DeviceNDArray</span><span class=p>)</span>
<a name=line-37></a><span class=k>def</span> <span class=nf>cuda_deserialize_numba_ndarray</span><span class=p>(</span><span class=n>header</span><span class=p>,</span> <span class=n>frames</span><span class=p>):</span>
<a name=line-38></a>    <span class=p>(</span><span class=n>frame</span><span class=p>,)</span> <span class=o>=</span> <span class=n>frames</span>
<a name=line-39></a>    <span class=n>shape</span> <span class=o>=</span> <span class=n>header</span><span class=p>[</span><span class=s2>&quot;shape&quot;</span><span class=p>]</span>
<a name=line-40></a>    <span class=n>strides</span> <span class=o>=</span> <span class=n>header</span><span class=p>[</span><span class=s2>&quot;strides&quot;</span><span class=p>]</span>
<a name=line-41></a>
<a name=line-42></a>    <span class=n>arr</span> <span class=o>=</span> <span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>devicearray</span><span class=o>.</span><span class=n>DeviceNDArray</span><span class=p>(</span>
<a name=line-43></a>        <span class=n>shape</span><span class=o>=</span><span class=n>shape</span><span class=p>,</span>
<a name=line-44></a>        <span class=n>strides</span><span class=o>=</span><span class=n>strides</span><span class=p>,</span>
<a name=line-45></a>        <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>dtype</span><span class=p>(</span><span class=n>header</span><span class=p>[</span><span class=s2>&quot;typestr&quot;</span><span class=p>]),</span>
<a name=line-46></a>        <span class=n>gpu_data</span><span class=o>=</span><span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>as_cuda_array</span><span class=p>(</span><span class=n>frame</span><span class=p>)</span><span class=o>.</span><span class=n>gpu_data</span><span class=p>,</span>
<a name=line-47></a>    <span class=p>)</span>
<a name=line-48></a>    <span class=k>return</span> <span class=n>arr</span>
<a name=line-49></a>
<a name=line-50></a>
<a name=line-51></a><span class=nd>@dask_serialize</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>devicearray</span><span class=o>.</span><span class=n>DeviceNDArray</span><span class=p>)</span>
<a name=line-52></a><span class=k>def</span> <span class=nf>dask_serialize_numba_ndarray</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
<a name=line-53></a>    <span class=n>header</span><span class=p>,</span> <span class=n>frames</span> <span class=o>=</span> <span class=n>cuda_serialize_numba_ndarray</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
<a name=line-54></a>    <span class=n>header</span><span class=p>[</span><span class=s2>&quot;writeable&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=kc>None</span><span class=p>,)</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>frames</span><span class=p>)</span>
<a name=line-55></a>    <span class=n>frames</span> <span class=o>=</span> <span class=p>[</span><span class=nb>memoryview</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>copy_to_host</span><span class=p>())</span> <span class=k>for</span> <span class=n>f</span> <span class=ow>in</span> <span class=n>frames</span><span class=p>]</span>
<a name=line-56></a>    <span class=k>return</span> <span class=n>header</span><span class=p>,</span> <span class=n>frames</span>
<a name=line-57></a>
<a name=line-58></a>
<a name=line-59></a><span class=nd>@dask_deserialize</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>devicearray</span><span class=o>.</span><span class=n>DeviceNDArray</span><span class=p>)</span>
<a name=line-60></a><span class=k>def</span> <span class=nf>dask_deserialize_numba_array</span><span class=p>(</span><span class=n>header</span><span class=p>,</span> <span class=n>frames</span><span class=p>):</span>
<a name=line-61></a>    <span class=k>if</span> <span class=n>dask_deserialize_rmm_device_buffer</span><span class=p>:</span>
<a name=line-62></a>        <span class=n>frames</span> <span class=o>=</span> <span class=p>[</span><span class=n>dask_deserialize_rmm_device_buffer</span><span class=p>(</span><span class=n>header</span><span class=p>,</span> <span class=n>frames</span><span class=p>)]</span>
<a name=line-63></a>    <span class=k>else</span><span class=p>:</span>
<a name=line-64></a>        <span class=n>frames</span> <span class=o>=</span> <span class=p>[</span><span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>to_device</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>asarray</span><span class=p>(</span><span class=nb>memoryview</span><span class=p>(</span><span class=n>f</span><span class=p>)))</span> <span class=k>for</span> <span class=n>f</span> <span class=ow>in</span> <span class=n>frames</span><span class=p>]</span>
<a name=line-65></a>        <span class=k>for</span> <span class=n>f</span> <span class=ow>in</span> <span class=n>frames</span><span class=p>:</span>
<a name=line-66></a>            <span class=n>weakref</span><span class=o>.</span><span class=n>finalize</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>numba</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_context</span><span class=p>)</span>
<a name=line-67></a>
<a name=line-68></a>    <span class=n>arr</span> <span class=o>=</span> <span class=n>cuda_deserialize_numba_ndarray</span><span class=p>(</span><span class=n>header</span><span class=p>,</span> <span class=n>frames</span><span class=p>)</span>
<a name=line-69></a>    <span class=k>return</span> <span class=n>arr</span>
</pre></div></td></tr></table><p class=footer> Generated by <a href=https://github.com/spirali/nedoc>nedoc</a> v0.9 at 2020-12-29 14:05 </p></div></div></body></html>